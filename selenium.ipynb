{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "f3cbb4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import undetected_chromedriver as uc\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "082414c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_fund(driver, wait, fund_name, index=0):\n",
    "    \"\"\"\n",
    "    Fills the specified fund search box with the given fund name and selects the matched suggestion.\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance.\n",
    "        wait: WebDriverWait instance.\n",
    "        fund_name: Name of the fund to search.\n",
    "        index: Index (0-4) for the respective fund search input box.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine correct input box ID\n",
    "        input_id = \"peer-fund-search\" if index == 0 else f\"peer-fund-search{index+1}\"\n",
    "        fund_input = wait.until(EC.presence_of_element_located((By.ID, input_id)))\n",
    "        \n",
    "        fund_input.click()\n",
    "        fund_input.clear()\n",
    "        fund_input.send_keys(fund_name)\n",
    "        print(f\"üîç Searching for: {fund_name} in #{input_id}\")\n",
    "\n",
    "        # Wait for suggestions\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".tt-dataset-fund_search .tt-suggestion\")))\n",
    "        time.sleep(5)  # Wait for suggestions to stabilize\n",
    "\n",
    "        # Get all suggestions\n",
    "        suggestions = driver.find_elements(By.CSS_SELECTOR, \".tt-dataset-fund_search .tt-suggestion\")\n",
    "\n",
    "        for suggestion in suggestions:\n",
    "            suggestion_text = suggestion.text.strip().lower()\n",
    "            if fund_name.lower().replace(\"direct\", \"dir\")[:15] in suggestion_text:\n",
    "                suggestion.click()\n",
    "                print(f\"‚úÖ Selected: {suggestion.text.strip()}\")\n",
    "                return\n",
    "        print(f\"‚ùå No matching suggestion found for: {fund_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in select_fund [{index}]: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "dde386e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_full_page_text(driver):\n",
    "#     try:\n",
    "#         body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "#         return body.text\n",
    "#     except Exception as e:\n",
    "#         print(\"‚ùå Error scraping page text:\", e)\n",
    "#         return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "dd839803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_comparison_table_basics(html: str) -> pd.DataFrame:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", {\"id\": \"peer-comparison-tab\"})\n",
    "    if not table:\n",
    "        raise ValueError(\"‚ùå 'peer-comparison-tab' table not found.\")\n",
    "\n",
    "    thead = table.find(\"thead\")\n",
    "    if thead is None:\n",
    "        raise ValueError(\"‚ùå Table header ('thead') not found.\")\n",
    "\n",
    "    header_cells = thead.find_all(\"th\")[1:]  # skip first empty cell\n",
    "    fund_names = [th.get_text(strip=True) for th in header_cells]\n",
    "\n",
    "    tbody = table.find(\"tbody\")\n",
    "    if tbody is None:\n",
    "        raise ValueError(\"‚ùå Table body ('tbody') not found.\")\n",
    "\n",
    "    rows = []\n",
    "    for tr in tbody.find_all(\"tr\"):\n",
    "        cells = tr.find_all(\"td\")\n",
    "        if len(cells) < len(fund_names) + 1:\n",
    "            continue\n",
    "        metric = cells[0].get_text(strip=True)\n",
    "        values = [td.get_text(strip=True) for td in cells[1:]]\n",
    "        rows.append([metric] + values)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"Metric\"] + fund_names)\n",
    "\n",
    "    mf_basics = df.copy()\n",
    "\n",
    "    if 'Metric' in mf_basics.columns:\n",
    "        mf_basics.set_index('Metric', inplace=True)\n",
    "\n",
    "    # Drop the unwanted rows\n",
    "    mf_basics_cleaned = mf_basics.drop([\"VR Rating\", \"Our Opinion\"])\n",
    "\n",
    "    # Transpose the DataFrame: rows ‚Üî columns\n",
    "    mf_basics_transposed = mf_basics_cleaned.transpose()\n",
    "\n",
    "    # Reset index (optional, for cleaner display)\n",
    "    mf_basics_transposed.reset_index(inplace=True)\n",
    "    mf_basics_transposed.rename(columns={\"index\": \"Fund Name\"}, inplace=True)\n",
    "\n",
    "    # Show the result\n",
    "    # print(mf_basics_transposed)\n",
    "\n",
    "    return mf_basics_transposed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "1fa857fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_trailing_returns_table(html: str) -> pd.DataFrame:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", {\"id\": \"trailingReturnTabs\"})\n",
    "    if not table:\n",
    "        raise ValueError(\"‚ùå Trailing Returns table not found.\")\n",
    "\n",
    "    # Extract headers\n",
    "    thead = table.find(\"thead\")\n",
    "    if not thead:\n",
    "        raise ValueError(\"‚ùå Table header ('thead') not found in trailing returns.\")\n",
    "\n",
    "    header_cells = thead.find_all(\"th\")[1:]  # skip first empty cell\n",
    "    fund_names = [th.get_text(strip=True) for th in header_cells]\n",
    "\n",
    "    # Extract body\n",
    "    tbody = table.find(\"tbody\")\n",
    "    if not tbody:\n",
    "        raise ValueError(\"‚ùå Table body ('tbody') not found in trailing returns.\")\n",
    "\n",
    "    rows = []\n",
    "    for tr in tbody.find_all(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "        if len(tds) < len(fund_names) + 1:\n",
    "            continue\n",
    "        metric = tds[0].get_text(strip=True)\n",
    "        values = [td.get_text(strip=True) for td in tds[1:]]\n",
    "        rows.append([metric] + values)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows, columns=[\"Return Period\"] + fund_names)\n",
    "    \n",
    "    # Set \"Return Period\" as index\n",
    "    df.set_index(\"Return Period\", inplace=True)\n",
    "\n",
    "    # Transpose and clean for easier comparison\n",
    "    trailing_returns = df.transpose().reset_index()\n",
    "    trailing_returns.rename(columns={\"index\": \"Fund Name\"}, inplace=True)\n",
    "\n",
    "    # print(trailing_returns)\n",
    "    return trailing_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "2e5e1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_risk_ratios_table(html: str) -> pd.DataFrame:\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\", {\"id\": \"riskRatiosTabs\"})\n",
    "    if not table:\n",
    "        raise ValueError(\"‚ùå Risk Ratios table not found.\")\n",
    "\n",
    "\n",
    "    # Try finding all rows in the table\n",
    "    rows = table.find_all(\"tr\")\n",
    "    if not rows:\n",
    "        raise ValueError(\"‚ùå No rows found in table.\")\n",
    "\n",
    "    # Find header row (the one with fund names)\n",
    "    header_row = rows[0]\n",
    "    header_cells = header_row.find_all(\"th\")[1:]  # Skip the first blank th\n",
    "    if not header_cells:\n",
    "        raise ValueError(\"‚ùå No header cells found.\")\n",
    "    \n",
    "    fund_names = [cell.get_text(strip=True) for cell in header_cells]\n",
    "\n",
    "    # Extract the data rows\n",
    "    data = []\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all([\"td\", \"th\"])\n",
    "        if len(cells) < len(fund_names) + 1:\n",
    "            continue  # Skip incomplete rows\n",
    "\n",
    "        metric_name = cells[0].get_text(strip=True)\n",
    "        values = [cell.get_text(strip=True) for cell in cells[1:]]\n",
    "        data.append([metric_name] + values)\n",
    "\n",
    "    # Form DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Metric\"] + fund_names)\n",
    "    df = df.set_index(\"Metric\").transpose().reset_index()\n",
    "    df = df.rename(columns={\"index\": \"Fund Name\"})\n",
    "\n",
    "    # print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "7ee06c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_asset_allocation_table(html: str) -> pd.DataFrame:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", {\"id\": \"asssetAllocationTabs\"})\n",
    "    if not table:\n",
    "        raise ValueError(\"‚ùå Asset Allocation table not found.\")\n",
    "\n",
    "    # Extract column headers (fund names)\n",
    "    headers = [th.get_text(strip=True) for th in table.find(\"thead\").find_all(\"th\")[1:]]  # skip first blank column\n",
    "\n",
    "    # Extract rows\n",
    "    row_labels = []\n",
    "    data = []\n",
    "    for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "        row_labels.append(tds[0].get_text(strip=True))\n",
    "        values = [td.get_text(strip=True) for td in tds[1:]]\n",
    "        data.append(values)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, index=row_labels, columns=headers)\n",
    "    df = df.transpose().reset_index()\n",
    "    df.rename(columns={\"index\": \"Fund Name\"}, inplace=True)\n",
    "    # print(df)\n",
    "    return df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "9fb4d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_sector_distribution_table(html: str) -> pd.DataFrame:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", {\"id\": \"sectorDistributionTabs\"})\n",
    "    if not table:\n",
    "        raise ValueError(\"‚ùå Sector Distribution table not found.\")\n",
    "\n",
    "    # Extract headers (fund names)\n",
    "    headers = [th.get_text(strip=True) for th in table.find(\"thead\").find_all(\"th\")[1:]]  # Skip first \"Sectors\"\n",
    "\n",
    "    # Extract rows\n",
    "    data = []\n",
    "    row_labels = []\n",
    "\n",
    "    for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "        sector_name = tds[0].get_text(strip=True)\n",
    "        values = [td.get_text(strip=True) for td in tds[1:]]\n",
    "        row_labels.append(sector_name)\n",
    "        data.append(values)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, index=row_labels, columns=headers)\n",
    "\n",
    "    df = df.transpose().reset_index()\n",
    "    df.rename(columns={\"index\": \"Fund Name\"}, inplace=True)\n",
    "\n",
    "    # print(df)\n",
    "    return df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "0fca02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_fund_holdings_summary_table(html: str) -> pd.DataFrame:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", {\"id\": \"holdingtables\"})\n",
    "    if not table:\n",
    "        raise ValueError(\"‚ùå Fund Holdings Summary table not found.\")\n",
    "\n",
    "    # Extract fund names from the <thead>\n",
    "    headers = [th.get_text(strip=True) for th in table.find(\"thead\").find_all(\"th\")[1:]]  # Skip first empty column\n",
    "\n",
    "    # Extract metric rows from the <tbody>\n",
    "    data = []\n",
    "    row_labels = []\n",
    "\n",
    "    for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "        metric_name = tds[0].get_text(strip=True)\n",
    "        values = [td.get_text(strip=True) for td in tds[1:]]\n",
    "        row_labels.append(metric_name)\n",
    "        data.append(values)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, index=row_labels, columns=headers)\n",
    "    df = df.transpose().reset_index()\n",
    "    df.rename(columns={\"index\": \"Fund Name\"}, inplace=True)\n",
    "\n",
    "    # print(df)\n",
    "    return df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "dcaf3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.chat_models import ChatOllama\n",
    "# from langchain.schema import SystemMessage, HumanMessage\n",
    "# import pandas as pd\n",
    "# import json\n",
    "\n",
    "# def extract_risk_table(html_snippet: str) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Uses LLaMA 3 via Ollama to convert scraped mutual fund HTML/text into a structured DataFrame.\n",
    "\n",
    "#     Parameters:\n",
    "#         html_snippet (str): Scraped text or HTML content of mutual fund comparison.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Structured data as a DataFrame.\n",
    "#     \"\"\"\n",
    "#     print(\"üîç Extracting mutual fund comparison table using LLaMA 3...\")\n",
    "#     llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "#         Below is raw text scraped from a mutual fund comparison webpage.\n",
    "\n",
    "#         Please return valid, complete json file :\n",
    "#         - Strictly return the JSON data without any additional text.\n",
    "#         - Each row should contain the fund names like Nippon India Large Cap Dir, HDFC Large Cap Dir those availabe on the html. No additional funds.\n",
    "#         - metrics like Mean (%), Std Dev (%), Sharpe, Sortino, Beta, Alpha should be captured in columns from the html page. No self calculated metrics. \n",
    "#         - Use only actual data. Do not use ellipses (...) or placeholders.\n",
    "#         - No explanations, no markdown, no comments, no print statements. Strictly follow this. Only return the json file.\n",
    "#         - Do not include Markdown formatting or text like 'Here is the JSON:, Here is the valid, complete JSON file: and all.\n",
    "#         - Don't generate any data from self. if data not available just appear it as it is.\n",
    "\n",
    "#         Raw data:\n",
    "#         ---\n",
    "#         {html_snippet[:3000]}\n",
    "#         ---\n",
    "#         \"\"\"\n",
    "\n",
    "#     messages = [\n",
    "#         SystemMessage(content=\"You are a helpful data scientist.\"),\n",
    "#         HumanMessage(content=prompt)\n",
    "#     ]\n",
    "\n",
    "#     response = llm.invoke(messages)\n",
    "\n",
    "#     try:\n",
    "#         data = json.loads(response.content.strip())\n",
    "#         df = pd.DataFrame(data)\n",
    "#         print(df)\n",
    "#         return df\n",
    "#     except Exception as e:\n",
    "#         raise RuntimeError(f\"‚ùå Failed to parse JSON from LLaMA output.\\nError: {e}\\nOutput:\\n{response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "2a762598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import undetected_chromedriver as uc\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Assumes select_fund and all extract_*_table functions are already defined\n",
    "\n",
    "def compare_and_extract_funds(funds_to_search):\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/117.0.0.0 Safari/537.36\")\n",
    "    prefs = {\n",
    "        \"credentials_enable_service\": False,\n",
    "        \"profile.password_manager_enabled\": False\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    driver = uc.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "\n",
    "    try:\n",
    "        # üîê LOGIN\n",
    "        driver.get(\"https://www.valueresearchonline.com/login\")\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(),'Log in with password')]\"))).click()\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"username\"))).send_keys(\"taswinikumar69@gmail.com\")\n",
    "        wait.until(EC.element_to_be_clickable((By.ID, \"proceed-btn\"))).click()\n",
    "        time.sleep(1)\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"login_password\"))).send_keys(\"Value*0321\")\n",
    "        wait.until(EC.element_to_be_clickable((By.ID, \"login-btn\"))).click()\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"navbarDropdown-my-investment\")))\n",
    "        print(\"‚úÖ Logged in successfully\")\n",
    "\n",
    "        # üîÑ FUND COMPARE\n",
    "        driver.get(\"https://www.valueresearchonline.com/funds/fund-compare/\")\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"navbarDropdown-my-investment\")))\n",
    "\n",
    "        for idx, fund in enumerate(funds_to_search[:5]):\n",
    "            select_fund(driver, wait, fund, index=idx)\n",
    "            time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            compare_btn = wait.until(EC.presence_of_element_located((By.ID, \"compare_fund\")))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", compare_btn)\n",
    "            time.sleep(1)\n",
    "            compare_btn.click()\n",
    "            print(\"‚úÖ Clicked 'Compare These Funds'\")\n",
    "        except Exception as e:\n",
    "            driver.execute_script(\"arguments[0].click();\", compare_btn)\n",
    "\n",
    "        time.sleep(4)\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "        # üìä BASIC & RETURN TABLES\n",
    "        html = driver.page_source\n",
    "        mf_basics = extract_comparison_table_basics(html)\n",
    "        mf_basics.to_csv(\"data/mf_basics.csv\", index=False)\n",
    "        print(\"‚úÖ Basic table saved\")\n",
    "\n",
    "        mf_return = extract_trailing_returns_table(html)\n",
    "        mf_return.to_csv(\"data/mf_return.csv\", index=False)\n",
    "        print(\"‚úÖ Return table saved\")\n",
    "\n",
    "        # üìâ RISK RATIOS\n",
    "        driver.execute_script(\"document.querySelector(\\\"a[href='#riskRatiosTab']\\\").click()\")\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, \"//table[@id='riskRatiosTabs']//tr[td]\")))\n",
    "        mf_risk = extract_risk_ratios_table(driver.page_source)\n",
    "        mf_risk.to_csv(\"data/mf_risk.csv\", index=False)\n",
    "        print(\"‚úÖ Risk Ratio table saved\")\n",
    "\n",
    "        # üìä ASSET ALLOCATION\n",
    "        driver.execute_script(\"document.querySelector(\\\"a[href='#asssetAllocationTab']\\\").click()\")\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, \"//table[@id='asssetAllocationTabs']//tr[td]\")))\n",
    "        mf_asset = extract_asset_allocation_table(driver.page_source)\n",
    "        mf_asset.to_csv(\"data/mf_asset_allocation.csv\", index=False)\n",
    "        print(\"‚úÖ Asset Allocation table saved\")\n",
    "\n",
    "        # üìä SECTOR DISTRIBUTION\n",
    "        driver.execute_script(\"document.querySelector(\\\"a[href='#sectorDistributionTab']\\\").click()\")\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, \"//table[@id='sectorDistributionTabs']//tr[td]\")))\n",
    "        df_sector = extract_sector_distribution_table(driver.page_source)\n",
    "        df_sector.to_csv(\"data/mf_sector_distribution.csv\", index=False)\n",
    "        print(\"‚úÖ Sector Distribution table saved\")\n",
    "\n",
    "        # üìã HOLDINGS\n",
    "        driver.execute_script(\"document.querySelector(\\\"a[href='#holdingsTab']\\\").click()\")\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, \"//table[@id='holdingtables']//tr[td]\")))\n",
    "        df_holdings = extract_fund_holdings_summary_table(driver.page_source)\n",
    "        df_holdings.to_csv(\"data/mf_holdings.csv\", index=False)\n",
    "        print(\"‚úÖ Holdings table saved\")\n",
    "\n",
    "        # ‚úÖ Merge all tables on 'Fund Name'\n",
    "        for df in [mf_basics, mf_return, mf_risk, mf_asset, df_sector, df_holdings]:\n",
    "            df.columns = df.columns.str.strip()  # Clean column names\n",
    "\n",
    "        merged_df = mf_basics \\\n",
    "            .merge(mf_return, on=\"Fund Name\", how=\"outer\") \\\n",
    "            .merge(mf_risk, on=\"Fund Name\", how=\"outer\") \\\n",
    "            .merge(mf_asset, on=\"Fund Name\", how=\"outer\") \\\n",
    "            .merge(df_sector, on=\"Fund Name\", how=\"outer\") \\\n",
    "            .merge(df_holdings, on=\"Fund Name\", how=\"outer\")\n",
    "\n",
    "        merged_df.to_csv(\"data/mf_merged.csv\", index=False)\n",
    "        print(\"‚úÖ Tables merged and saved successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error:\", e)\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a203717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged in successfully\n",
      "üîç Searching for: Nippon India Large Cap Fund Dir in #peer-fund-search\n",
      "‚ùå No matching suggestion found for: Nippon India Large Cap Fund Dir\n",
      "üîç Searching for: HDFC Large Cap Dir in #peer-fund-search2\n",
      "‚úÖ Selected: \n",
      "‚úÖ Basic table saved\n",
      "‚úÖ Return table saved\n",
      "‚úÖ Risk Ratio table saved\n",
      "‚úÖ Asset Allocation table saved\n",
      "‚úÖ Sector Distribution table saved\n",
      "‚úÖ Holdings table saved\n",
      "‚úÖ Tables merged and saved successfully\n"
     ]
    }
   ],
   "source": [
    "fund_list = [\n",
    "        \"Nippon India Large Cap Fund Dir\",\n",
    "        \"HDFC Large Cap Dir\"\n",
    "    ]\n",
    "compare_and_extract_funds(fund_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654f42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
